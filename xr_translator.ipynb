{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b5327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "xr.set_options(display_expand_data=False)\n",
    "\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f8cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSIDs:\n",
    "TSIDs = ['BESI', 'GREGOR', 'HOP', 'JIL', 'NICHTJIL', 'SOM', \n",
    "         'TEST-HOP', 'TEST-HOP-mgn', 'TEST-SOM-mgn', 'TEST-VST',\n",
    "         'TS021', 'TS025', 'TS027', 'TS028', 'TS029', 'VST']\n",
    "\n",
    "#Data File Path:\n",
    "file_path = os.path.join(glob.glob('C:\\\\Users\\\\mvomstein\\\\projects\\\\vt-2pod')[0], 'DATA')\n",
    "\n",
    "file_path_save = glob.glob(\n",
    "    'C:\\\\Users\\\\mvomstein\\\\projects\\\\vt_2pod_data_analysis\\\\DATA_xr')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "189c8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = np.loadtxt(os.path.join(file_path,'survey_2023-01-05.tsv'),\n",
    "                 delimiter=\"\\t\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64af1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSs = ['forearm', 'back-lower', 'back-lower-nsa', 'abdomen', 'thigh']\n",
    "HVs = ['h', 'v']\n",
    "\n",
    "bsDic = {'forearm': 0,\n",
    "         'back-lower': 1,\n",
    "         'back-lower-nsa': 2,\n",
    "         'abdomen': 3,\n",
    "         'thigh': 4}\n",
    "\n",
    "hvDic = {'h': 0,\n",
    "         'v': 1}\n",
    "\n",
    "sexDic = {'F': 0, 'M': 1}\n",
    "\n",
    "medicalDic = {0 : 'Keine',\n",
    "              1 : 'Diabetes',\n",
    "              2 : 'Kapaltunnel',\n",
    "              3 : 'Neurodermitis',\n",
    "              4 : 'Taktile Wahrnehmungsveraenderung',\n",
    "              5 : 'ADHS',\n",
    "              6 : 'Lernstoerung',\n",
    "              7 : 'Erkrankung ZNS',\n",
    "              8 : 'Beeintraechtigung Sehsinn'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a421d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatch(param, fpts, bs, hv, datatype):\n",
    "    matches = [match for match in os.listdir(fpts) \n",
    "           if bs in match \n",
    "           and hv in match \n",
    "           and param in match\n",
    "           and os.path.splitext(match)[1] == datatype\n",
    "          ]\n",
    "    #print(matches)\n",
    "    if len(matches) >=2: raise Exception(f'MULTIPLE ENTRIES IN {TSID}: DATA NOT CLEAN', matches)\n",
    "    return(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc731497",
   "metadata": {},
   "outputs": [],
   "source": [
    "xStr = ['postmean', 'poststd', 'stim', 'response', 'pGuess', 'pSlope', 'pThresh']\n",
    "yVal = [np.arange(2.5, 61, 2.5),\n",
    "        np.arange(2.5, 61, 2.5),\n",
    "        np.arange(0, 50, 1),\n",
    "        np.arange(0, 50, 1),\n",
    "        np.linspace(0.01, 0.99, 100), #guess\n",
    "        np.linspace(0.01, 10, 50), #slope\n",
    "        np.linspace(0.01, 60, 31) #thresh\n",
    "       ]\n",
    "yStr = ['stimRange', 'stimRange', 'trial', 'trial', 'guessGrid', 'slopeGrid', 'threshGrid']\n",
    "\n",
    "for i in range(0, len(xStr), 1):\n",
    "    for TSID in TSIDs:\n",
    "        fpTS = os.path.join(file_path, TSID)\n",
    "        arr = np.random.normal(size=(len(yVal[i]),1,5,2))*np.nan\n",
    "        for BS, HV in itertools.product(BSs, HVs):\n",
    "            matches = getMatch('_'+xStr[i]+'_', fpTS, '_'+BS+'_', '_'+HV+'_', '.npy')\n",
    "            if matches:\n",
    "                x = re.split(\"_\", matches[0])\n",
    "                singleData =np.load(\n",
    "                    os.path.join(fpTS, matches[0]), allow_pickle=True)\n",
    "                arr[:len(singleData),0, bsDic[x[1]], hvDic[x[2]]] = singleData\n",
    "\n",
    "                #x[0]: TSID, x[1]: bs, x[2]: hv\n",
    "\n",
    "        da = xr.DataArray(\n",
    "            arr,\n",
    "            dims=[yStr[i], 'tsid', 'bs', 'hv'],\n",
    "            coords={yStr[i]:yVal[i],\n",
    "                    'tsid':[TSID], 'bs':BSs, 'hv':['h', 'v']},\n",
    "            name=xStr[i]\n",
    "        )\n",
    "        da.to_netcdf(os.path.join(file_path_save, xStr[i], xStr[i]+TSID+'.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fee09c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eParams\n",
    "# =============================================================================\n",
    "\n",
    "yStr = ['eGuess', 'eLapse', 'eSlope', 'eThreshold', \n",
    "        'stdGuess', 'stdLapse', 'stdSlope', 'stdThreshold']\n",
    "\n",
    "for TSID in TSIDs:\n",
    "    fpTS = os.path.join(file_path, TSID)\n",
    "    arr = np.random.normal(size=(8,1,5,2))*np.nan\n",
    "    for BS, HV, i in itertools.product(BSs, HVs, range(0, len(yStr))):\n",
    "        matches = getMatch('_'+yStr[i]+'_', fpTS, '_'+BS+'_', '_'+HV+'_', '.npy')\n",
    "        if matches:\n",
    "            x = re.split(\"_\", matches[0])\n",
    "            singleData =np.load(\n",
    "                os.path.join(fpTS, matches[0]), allow_pickle=True)\n",
    "            arr[i ,0, bsDic[x[1]], hvDic[x[2]]] = singleData\n",
    "\n",
    "                #x[0]: TSID, x[1]: bs, x[2]: hv\n",
    "\n",
    "    da = xr.DataArray(\n",
    "        arr,\n",
    "        dims=['variableEP', 'tsid', 'bs', 'hv'],\n",
    "        coords={'variableEP':yStr,\n",
    "                'tsid':[TSID], 'bs':BSs, 'hv':['h', 'v']},\n",
    "        name='eParams'\n",
    "    )\n",
    "    da.to_netcdf(os.path.join(file_path_save, 'eParams', 'eParams'+TSID+'.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "515396f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "CHONK AND HAIR MISSING FOR [['GREGOR', 'forearm', 'h'], ['GREGOR', 'forearm', 'v'], ['JIL', 'forearm', 'h'], ['TEST-HOP', 'forearm', 'h'], ['TEST-HOP', 'back-lower', 'h'], ['TEST-HOP', 'back-lower', 'v'], ['TEST-HOP-mgn', 'back-lower', 'v'], ['TEST-HOP-mgn', 'back-lower-nsa', 'h'], ['TEST-SOM-mgn', 'back-lower', 'h'], ['TEST-SOM-mgn', 'back-lower', 'v'], ['TEST-VST', 'back-lower', 'h'], ['TEST-VST', 'back-lower', 'v'], ['TEST-VST', 'back-lower-nsa', 'h']]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6280\\1116221958.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tsMeta'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tsMeta'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mTSID\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.nc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mchonk_missing\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'CHONK AND HAIR MISSING FOR {chonk_missing}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msex_missing\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'TS META MISSING FOR {sex_missing}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: CHONK AND HAIR MISSING FOR [['GREGOR', 'forearm', 'h'], ['GREGOR', 'forearm', 'v'], ['JIL', 'forearm', 'h'], ['TEST-HOP', 'forearm', 'h'], ['TEST-HOP', 'back-lower', 'h'], ['TEST-HOP', 'back-lower', 'v'], ['TEST-HOP-mgn', 'back-lower', 'v'], ['TEST-HOP-mgn', 'back-lower-nsa', 'h'], ['TEST-SOM-mgn', 'back-lower', 'h'], ['TEST-SOM-mgn', 'back-lower', 'v'], ['TEST-VST', 'back-lower', 'h'], ['TEST-VST', 'back-lower', 'v'], ['TEST-VST', 'back-lower-nsa', 'h']]"
     ]
    }
   ],
   "source": [
    "# tsMeta\n",
    "# =============================================================================\n",
    "\n",
    "yStr = ['sex', 'age', 'height', 'weight', 'medical', 'chonk', 'hair']\n",
    "\n",
    "sex_missing = []\n",
    "chonk_missing = []\n",
    "for TSID in TSIDs:\n",
    "    fpTS = os.path.join(file_path, TSID)\n",
    "    arr = np.random.normal(size=(len(yStr),1,5,2))*np.nan\n",
    "    \n",
    "    # sex, age, height, weight, sickness\n",
    "    # --------------------------------------------------------------------------\n",
    "    if any(np.where(survey==TSID)[0]):\n",
    "        sex_etc = survey[np.where(survey==TSID)[0][0]][3:8]\n",
    "        arr[:5, 0, :, [0,1]] = [sexDic[sex_etc[0]],\n",
    "                                int(sex_etc[1]), \n",
    "                                int(sex_etc[2]), \n",
    "                                int(sex_etc[3]),\n",
    "                                int(sex_etc[4][-2])]\n",
    "    else:\n",
    "        sex_missing.append([TSID])\n",
    "        \n",
    "    # chonk, hair\n",
    "    # --------------------------------------------------------------------------\n",
    "    for BS, HV in itertools.product(BSs, HVs):\n",
    "        matches = getMatch('_meta_', fpTS, '_'+BS+'_', '_'+HV+'_', '.txt')\n",
    "        if matches:\n",
    "            x = re.split(\"_\", matches[0])\n",
    "            chonk, hair = np.genfromtxt(fpTS+'/'+matches[0], dtype='str', skip_header=True)[1:,1]\n",
    "            arr[-2:, 0, bsDic[x[1]], hvDic[x[2]]] = chonk, hair\n",
    "            \n",
    "        if not matches and getMatch('postmean', fpTS, '_'+BS+'_', '_'+HV+'_', '.npy'):\n",
    "            chonk_missing.append([TSID, BS, HV]) #f√ºr manuelles nachtragen\n",
    "    # --------------------------------------------------------------------------\n",
    "        #x[0]: TSID, x[1]: bs, x[2]: hv\n",
    "\n",
    "    da = xr.DataArray(\n",
    "        arr,\n",
    "        dims=['variableTSM', 'tsid', 'bs', 'hv'],\n",
    "        coords={'variableTSM':yStr,\n",
    "                'tsid':[TSID], 'bs':BSs, 'hv':['h', 'v']},\n",
    "        name='tsMeta'\n",
    "    )\n",
    "    da.to_netcdf(os.path.join(file_path_save, 'tsMeta', 'tsMeta'+TSID+'.nc'))\n",
    "    \n",
    "if chonk_missing: raise Exception(f'CHONK AND HAIR MISSING FOR {chonk_missing}')\n",
    "if sex_missing: raise Exception(f'TS META MISSING FOR {sex_missing}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
