{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b5327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "xr.set_options(display_expand_data=False)\n",
    "\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f8cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSIDs:\n",
    "TSIDs = ['BESI', 'GREGOR', 'HOP', 'JIL', 'NICHTJIL', 'SOM', \n",
    "         'TEST-HOP', 'TEST-HOP-mgn', 'TEST-SOM-mgn', 'TEST-VST',\n",
    "         'TS021', 'TS025', 'TS027', 'TS028', 'TS029', 'VST']\n",
    "\n",
    "#Data File Path:\n",
    "file_path = os.path.join(glob.glob('C:\\\\Users\\\\mvomstein\\\\projects\\\\vt-2pod')[0], 'DATA')\n",
    "\n",
    "file_path_save = glob.glob(\n",
    "    'C:\\\\Users\\\\mvomstein\\\\projects\\\\vt_2pod_data_analysis\\\\DATA_xr')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189c8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = np.loadtxt(os.path.join(file_path,'survey_2023-01-05.tsv'),\n",
    "                 delimiter=\"\\t\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64af1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSs = ['forearm', 'back-lower', 'back-lower-nsa', 'abdomen', 'thigh']\n",
    "HVs = ['h', 'v']\n",
    "\n",
    "bsDic = {'forearm': 0,\n",
    "         'back-lower': 1,\n",
    "         'back-lower-nsa': 2,\n",
    "         'abdomen': 3,\n",
    "         'thigh': 4}\n",
    "\n",
    "hvDic = {'h': 0,\n",
    "         'v': 1}\n",
    "\n",
    "sexDic = {'F': 0, 'M': 1}\n",
    "\n",
    "medicalDic = {0 : 'Keine',\n",
    "              1 : 'Diabetes',\n",
    "              2 : 'Kapaltunnel',\n",
    "              3 : 'Neurodermitis',\n",
    "              4 : 'Taktile Wahrnehmungsveraenderung',\n",
    "              5 : 'ADHS',\n",
    "              6 : 'Lernstoerung',\n",
    "              7 : 'Erkrankung ZNS',\n",
    "              8 : 'Beeintraechtigung Sehsinn'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a421d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatch(param, fpts, bs, hv, datatype):\n",
    "    matches = [match for match in os.listdir(fpts) \n",
    "           if bs in match \n",
    "           and hv in match \n",
    "           and param in match\n",
    "           and os.path.splitext(match)[1] == datatype\n",
    "          ]\n",
    "    #print(matches)\n",
    "    if len(matches) >=2: raise Exception(f'MULTIPLE ENTRIES IN {TSID}: DATA NOT CLEAN', matches)\n",
    "    return(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc731497",
   "metadata": {},
   "outputs": [],
   "source": [
    "xStr = ['postmean', 'poststd', 'stim', 'response', 'pGuess', 'pSlope', 'pThresh']\n",
    "yVal = [np.arange(2.5, 61, 2.5),\n",
    "        np.arange(2.5, 61, 2.5),\n",
    "        np.arange(0, 50, 1),\n",
    "        np.arange(0, 50, 1),\n",
    "        np.linspace(0.01, 0.99, 100), #guess\n",
    "        np.linspace(0.01, 10, 50), #slope\n",
    "        np.linspace(0.01, 60, 31) #thresh\n",
    "       ]\n",
    "yStr = ['stimRange', 'stimRange', 'trial', 'trial', 'guessGrid', 'slopeGrid', 'threshGrid']\n",
    "\n",
    "for i in range(0, len(xStr), 1):\n",
    "    for TSID in TSIDs:\n",
    "        fpTS = os.path.join(file_path, TSID)\n",
    "        arr = np.random.normal(size=(len(yVal[i]),1,5,2))*np.nan\n",
    "        for BS, HV in itertools.product(BSs, HVs):\n",
    "            matches = getMatch('_'+xStr[i]+'_', fpTS, '_'+BS+'_', '_'+HV+'_', '.npy')\n",
    "            if matches:\n",
    "                x = re.split(\"_\", matches[0])\n",
    "                singleData =np.load(\n",
    "                    os.path.join(fpTS, matches[0]), allow_pickle=True)\n",
    "                arr[:len(singleData),0, bsDic[x[1]], hvDic[x[2]]] = singleData\n",
    "\n",
    "                #x[0]: TSID, x[1]: bs, x[2]: hv\n",
    "\n",
    "        da = xr.DataArray(\n",
    "            arr,\n",
    "            dims=[yStr[i], 'tsid', 'bs', 'hv'],\n",
    "            coords={yStr[i]:yVal[i],\n",
    "                    'tsid':[TSID], 'bs':BSs, 'hv':['h', 'v']},\n",
    "            name=xStr[i]\n",
    "        )\n",
    "        da.to_netcdf(os.path.join(file_path_save, xStr[i], xStr[i]+TSID+'.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fee09c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eParams\n",
    "# =============================================================================\n",
    "\n",
    "yStr = ['eGuess', 'eLapse', 'eSlope', 'eThreshold', \n",
    "        'stdGuess', 'stdLapse', 'stdSlope', 'stdThreshold']\n",
    "\n",
    "for TSID in TSIDs:\n",
    "    fpTS = os.path.join(file_path, TSID)\n",
    "    arr = np.random.normal(size=(8,1,5,2))*np.nan\n",
    "    for BS, HV, i in itertools.product(BSs, HVs, range(0, len(yStr))):\n",
    "        matches = getMatch('_'+yStr[i]+'_', fpTS, '_'+BS+'_', '_'+HV+'_', '.npy')\n",
    "        if matches:\n",
    "            x = re.split(\"_\", matches[0])\n",
    "            singleData =np.load(\n",
    "                os.path.join(fpTS, matches[0]), allow_pickle=True)\n",
    "            arr[i ,0, bsDic[x[1]], hvDic[x[2]]] = singleData\n",
    "\n",
    "                #x[0]: TSID, x[1]: bs, x[2]: hv\n",
    "\n",
    "    da = xr.DataArray(\n",
    "        arr,\n",
    "        dims=['variableEP', 'tsid', 'bs', 'hv'],\n",
    "        coords={'variableEP':yStr,\n",
    "                'tsid':[TSID], 'bs':BSs, 'hv':['h', 'v']},\n",
    "        name='eParams'\n",
    "    )\n",
    "    da.to_netcdf(os.path.join(file_path_save, 'eParams', 'eParams'+TSID+'.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "515396f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsMeta\n",
    "# =============================================================================\n",
    "\n",
    "yStr = ['sex', 'age', 'height', 'weight', 'medical', 'chonk', 'hair']\n",
    "\n",
    "sex_missing = []\n",
    "chonk_missing = []\n",
    "for TSID in TSIDs:\n",
    "    fpTS = os.path.join(file_path, TSID)\n",
    "    arr = np.random.normal(size=(len(yStr),1,5,2))*np.nan\n",
    "    \n",
    "    # sex, age, height, weight, sickness\n",
    "    # --------------------------------------------------------------------------\n",
    "    if any(np.where(survey==TSID)[0]):\n",
    "        sex_etc = survey[np.where(survey==TSID)[0][0]][3:8]\n",
    "        arr[:5, 0, :, [0,1]] = [sexDic[sex_etc[0]],\n",
    "                                int(sex_etc[1]), \n",
    "                                int(sex_etc[2]), \n",
    "                                int(sex_etc[3]),\n",
    "                                int(sex_etc[4][-2])]\n",
    "    else:\n",
    "        sex_missing.append([TSID])\n",
    "        \n",
    "    # chonk, hair\n",
    "    # --------------------------------------------------------------------------\n",
    "    for BS, HV in itertools.product(BSs, HVs):\n",
    "        matches = getMatch('_meta_', fpTS, '_'+BS+'_', '_'+HV+'_', '.txt')\n",
    "        if matches:\n",
    "            x = re.split(\"_\", matches[0])\n",
    "            chonk, hair = np.genfromtxt(fpTS+'/'+matches[0], dtype='str', skip_header=True)[1:,1]\n",
    "            arr[-2:, 0, bsDic[x[1]], hvDic[x[2]]] = chonk, hair\n",
    "            \n",
    "        if not matches and getMatch('postmean', fpTS, '_'+BS+'_', '_'+HV+'_', '.npy'):\n",
    "            chonk_missing.append([TSID, BS, HV]) #f√ºr manuelles nachtragen\n",
    "    # --------------------------------------------------------------------------\n",
    "        #x[0]: TSID, x[1]: bs, x[2]: hv\n",
    "\n",
    "    da = xr.DataArray(\n",
    "        arr,\n",
    "        dims=['variableTSM', 'tsid', 'bs', 'hv'],\n",
    "        coords={'variableTSM':yStr,\n",
    "                'tsid':[TSID], 'bs':BSs, 'hv':['h', 'v']},\n",
    "        name='tsMeta'\n",
    "    )\n",
    "    da.to_netcdf(os.path.join(file_path_save, 'tsMeta', 'tsMeta'+TSID+'.nc'))\n",
    "    \n",
    "if chonk_missing: raise Exception(f'CHONK AND HAIR MISSING FOR {chonk_missing}')\n",
    "if sex_missing: raise Exception(f'TS META MISSING FOR {sex_missing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e58e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing meta generator\n",
    "# =============================================================================\n",
    "for coords in chonk_missing:\n",
    "    matches = getMatch('_postmean_', os.path.join(file_path, coords[0]),\n",
    "                       coords[1]+'_', coords[2], '.npy')\n",
    "    timestamp = re.split(\"_\", matches[0])[-2]+'_'+ re.split(\"_\", matches[0])[-1][:-4]\n",
    "    \n",
    "    print(coords)\n",
    "    chonk = input('chonk: ')\n",
    "    hair = input('hair: ')\n",
    "    \n",
    "    with open(os.path.join(\n",
    "       file_path, coords[0], coords[0] +\"_\"+ coords[1] \n",
    "        + \"_\" + coords[2] +'_meta_'+timestamp+'.txt'), 'w') as f:\n",
    "       f.write(coords[0]+'\\n')\n",
    "       f.write(coords[1]+'  '+coords[2]+ '\\n\\n')\n",
    "       f.write('chonk: '+chonk)\n",
    "       f.write('\\n')\n",
    "       f.write('hair: '+hair)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
